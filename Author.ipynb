{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from IPython.display import display\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./TRAIN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "from nltk.corpus import stopwords \n",
    "#Clean the text by retaining only alphabets and converting all the characters to small\n",
    "def clean_up(review):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \",review)\n",
    "    #replace multiple space by a single\n",
    "    clean = re.sub(' +', ' ',clean)\n",
    "    \n",
    "    word_tokens= clean.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    le=WordNetLemmatizer()\n",
    "    stop_words= set(stopwords.words(\"english\"))     \n",
    "    word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    cleaned =\" \".join(word_tokens)\n",
    "    #re.sub(' +', ' ',string4)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['cleaned'] = data['text'].apply(clean_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They have been pronounced by an\\r\\n\\r\\n\\r\\n\\r\\...</td>\n",
       "      <td>2</td>\n",
       "      <td>pronounced expert rare variety considerable va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>His partner sailed along in\\r\\n\\r\\n\\r\\n\\r\\n\\r\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>partner sailed along front though noticed noth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The cushions were a good deal higher\\r\\n\\r\\n\\r...</td>\n",
       "      <td>5</td>\n",
       "      <td>cushion good deal higher ball ball fashion alw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author  \\\n",
       "0  They have been pronounced by an\\r\\n\\r\\n\\r\\n\\r\\...       2   \n",
       "1  His partner sailed along in\\r\\n\\r\\n\\r\\n\\r\\n\\r\\...       0   \n",
       "2  The cushions were a good deal higher\\r\\n\\r\\n\\r...       5   \n",
       "\n",
       "                                             cleaned  \n",
       "0  pronounced expert rare variety considerable va...  \n",
       "1  partner sailed along front though noticed noth...  \n",
       "2  cushion good deal higher ball ball fashion alw...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleaned</th>\n",
       "      <td>They have been pronounced by an expert to be o...</td>\n",
       "      <td>His partner sailed along in front of him as th...</td>\n",
       "      <td>The cushions were a good deal higher than the ...</td>\n",
       "      <td>O God grant that in his presence I may rather ...</td>\n",
       "      <td>The grass glowed with bright and fragrant flow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0  \\\n",
       "author                                                   2   \n",
       "cleaned  They have been pronounced by an expert to be o...   \n",
       "\n",
       "                                                         1  \\\n",
       "author                                                   0   \n",
       "cleaned  His partner sailed along in front of him as th...   \n",
       "\n",
       "                                                         2  \\\n",
       "author                                                   5   \n",
       "cleaned  The cushions were a good deal higher than the ...   \n",
       "\n",
       "                                                         3  \\\n",
       "author                                                   4   \n",
       "cleaned  O God grant that in his presence I may rather ...   \n",
       "\n",
       "                                                         4  \n",
       "author                                                   0  \n",
       "cleaned  The grass glowed with bright and fragrant flow...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = ['author','cleaned']\n",
    "display_all(data[l].head().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>They have been pronounced by an expert to be o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>His partner sailed along in front of him as th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>The cushions were a good deal higher than the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>O God grant that in his presence I may rather ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author                                            cleaned\n",
       "0       2  They have been pronounced by an expert to be o...\n",
       "1       0  His partner sailed along in front of him as th...\n",
       "2       5  The cushions were a good deal higher than the ...\n",
       "3       4  O God grant that in his presence I may rather ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[l].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       object\n",
       "author      int64\n",
       "cleaned    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18977, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n",
    "#18,977 training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Text_Length'] = data['cleaned'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>Text_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They have been pronounced by an\\r\\n\\r\\n\\r\\n\\r\\...</td>\n",
       "      <td>2</td>\n",
       "      <td>pronounced expert rare variety considerable va...</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>His partner sailed along in\\r\\n\\r\\n\\r\\n\\r\\n\\r\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>partner sailed along front though noticed noth...</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The cushions were a good deal higher\\r\\n\\r\\n\\r...</td>\n",
       "      <td>5</td>\n",
       "      <td>cushion good deal higher ball ball fashion alw...</td>\n",
       "      <td>1305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O God, grant that in his presence I may\\r\\n\\r\\...</td>\n",
       "      <td>4</td>\n",
       "      <td>god grant presence may rather see vileness beh...</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The grass\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nglowed with brigh...</td>\n",
       "      <td>0</td>\n",
       "      <td>grass glowed bright fragrant flower bird flyin...</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author  \\\n",
       "0  They have been pronounced by an\\r\\n\\r\\n\\r\\n\\r\\...       2   \n",
       "1  His partner sailed along in\\r\\n\\r\\n\\r\\n\\r\\n\\r\\...       0   \n",
       "2  The cushions were a good deal higher\\r\\n\\r\\n\\r...       5   \n",
       "3  O God, grant that in his presence I may\\r\\n\\r\\...       4   \n",
       "4  The grass\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nglowed with brigh...       0   \n",
       "\n",
       "                                             cleaned  Text_Length  \n",
       "0  pronounced expert rare variety considerable va...          415  \n",
       "1  partner sailed along front though noticed noth...          793  \n",
       "2  cushion good deal higher ball ball fashion alw...         1305  \n",
       "3  god grant presence may rather see vileness beh...          652  \n",
       "4  grass glowed bright fragrant flower bird flyin...          648  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3859\n",
       "5    3517\n",
       "4    3184\n",
       "2    2621\n",
       "3    1314\n",
       "9    1142\n",
       "7    1054\n",
       "8     910\n",
       "6     755\n",
       "1     621\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#raw_valid.ProductSize.value_counts().plot.barh();\n",
    "data.author.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nowwASeooA0CSOur/A1N0c5M4Fu79AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0ff92f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Look at the distribution of the class among the 10 classes\n",
    "import matplotlib.pyplot as plt\n",
    "data.author.value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author  0  924\n",
      "Author  1  1742\n",
      "Author  2  936\n",
      "Author  3  1716\n",
      "Author  4  1293\n",
      "Author  5  1208\n",
      "Author  6  1981\n",
      "Author  7  1786\n",
      "Author  8  1656\n",
      "Author  9  1031\n"
     ]
    }
   ],
   "source": [
    "#Average count of words for different words\n",
    "for i in range(0,10):\n",
    "    print('Author',\"\",i,\"\",math.ceil(data[data['author']==i]['Text_Length'].mean()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_snippets = data['cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model, A Keras NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virajdattkohir/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create vectors of the text\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_snippets) # only fit on train\n",
    "snippet_x_train = tokenize.texts_to_matrix(train_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One hot encode the outcome classes\n",
    "snippet_y_train = utils.to_categorical(data['author'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18977, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18977, 1000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 details\n",
    "* max_words = 1000\n",
    "* batch_size = 32\n",
    "* epochs = 3\n",
    "* First layer 512 units\n",
    "* Dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15181 samples, validate on 3796 samples\n",
      "Epoch 1/4\n",
      "15181/15181 [==============================] - 2s 134us/step - loss: 0.6997 - acc: 0.7833 - val_loss: 0.3471 - val_acc: 0.8865\n",
      "Epoch 2/4\n",
      "15181/15181 [==============================] - 2s 117us/step - loss: 0.2712 - acc: 0.9132 - val_loss: 0.3012 - val_acc: 0.9054\n",
      "Epoch 3/4\n",
      "15181/15181 [==============================] - 2s 125us/step - loss: 0.1707 - acc: 0.9468 - val_loss: 0.3011 - val_acc: 0.9025\n",
      "Epoch 4/4\n",
      "15181/15181 [==============================] - 2s 126us/step - loss: 0.1192 - acc: 0.9634 - val_loss: 0.3036 - val_acc: 0.9009\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "# Build the model 1\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(snippet_x_train, snippet_y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.2 s, sys: 453 ms, total: 8.65 s\n",
      "Wall time: 8.91 s\n"
     ]
    }
   ],
   "source": [
    "#Read and clean test data\n",
    "test_df =  pd.read_csv(\"./TEST (1).csv\")\n",
    "%time test_df['cleaned'] = test_df['text'].apply(clean_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What though I was the victim of\\r\\r\\r\\r\\r\\n   ...</td>\n",
       "      <td>What though I was the victim of an extraordina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Was ever such a cock-and-bull story in this li...</td>\n",
       "      <td>Was ever such a cock and bull story in this li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does Alice appear to be present when she is\\r\\...</td>\n",
       "      <td>Does Alice appear to be present when she is ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She inspired the men she knew\\r\\r\\r\\r\\r\\n     ...</td>\n",
       "      <td>She inspired the men she knew with feelings sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  What though I was the victim of\\r\\r\\r\\r\\r\\n   ...   \n",
       "1  Was ever such a cock-and-bull story in this li...   \n",
       "2  Does Alice appear to be present when she is\\r\\...   \n",
       "3  She inspired the men she knew\\r\\r\\r\\r\\r\\n     ...   \n",
       "\n",
       "                                             cleaned  \n",
       "0  What though I was the victim of an extraordina...  \n",
       "1  Was ever such a cock and bull story in this li...  \n",
       "2  Does Alice appear to be present when she is ab...  \n",
       "3  She inspired the men she knew with feelings sh...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make vectors of the text from the test data\n",
    "test_snippets = test_df['cleaned']\n",
    "snippet_x_test = tokenize.texts_to_matrix(test_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6326, 1100)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make predcitions \n",
    "predictions_are = model.predict_classes(snippet_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 2, 4, ..., 0, 2, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the excel subimission file \n",
    "pd.DataFrame(predictions_are).to_excel(\"./subimission_01.xlsx\")\n",
    "# 0.908156813 model 1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 performance \n",
    "*  Val_Acc is 91.04\n",
    "*  Score on submission 0.908156813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 details\n",
    "* max_words = 1100\n",
    "* batch_size = 32\n",
    "* epochs = 6\n",
    "* First layer 512 units \n",
    "* Dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15181 samples, validate on 3796 samples\n",
      "Epoch 1/6\n",
      "15181/15181 [==============================] - 2s 136us/step - loss: 0.5852 - acc: 0.8195 - val_loss: 0.3466 - val_acc: 0.8883\n",
      "Epoch 2/6\n",
      "15181/15181 [==============================] - 2s 115us/step - loss: 0.2174 - acc: 0.9318 - val_loss: 0.3031 - val_acc: 0.8986\n",
      "Epoch 3/6\n",
      "15181/15181 [==============================] - 2s 115us/step - loss: 0.1256 - acc: 0.9613 - val_loss: 0.3237 - val_acc: 0.8933\n",
      "Epoch 4/6\n",
      "15181/15181 [==============================] - 2s 116us/step - loss: 0.0687 - acc: 0.9820 - val_loss: 0.3415 - val_acc: 0.8949\n",
      "Epoch 5/6\n",
      "15181/15181 [==============================] - 2s 115us/step - loss: 0.0354 - acc: 0.9935 - val_loss: 0.3686 - val_acc: 0.8970\n",
      "Epoch 6/6\n",
      "15181/15181 [==============================] - 2s 118us/step - loss: 0.0209 - acc: 0.9980 - val_loss: 0.3761 - val_acc: 0.8994\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 6\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(512, input_shape=(1000,)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(10))#number of outputs\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model2.fit(snippet_x_train, snippet_y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_are_2 = model2.predict_classes(snippet_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 600)               660600    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                6010      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 666,610\n",
      "Trainable params: 666,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 517,642\n",
      "Trainable params: 517,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 4, ..., 0, 2, 2])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_are_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(predictions_are_2).to_excel(\"./subimission_02.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 performance\n",
    "* Val_Acc is 91.12\n",
    "* Score on submission 0.90303824\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = np.subtract(predictions_are,predictions_are_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    7,   29,   30,   36,   55,   67,  140,  157,  180,  191,\n",
       "         232,  245,  258,  266,  272,  279,  333,  339,  348,  352,  393,\n",
       "         396,  411,  424,  427,  446,  451,  465,  489,  519,  570,  585,\n",
       "         616,  622,  640,  652,  655,  664,  694,  710,  720,  751,  754,\n",
       "         806,  842,  846,  862,  881,  896,  903,  920,  922,  929,  934,\n",
       "         961,  973, 1006, 1033, 1034, 1055, 1102, 1127, 1138, 1157, 1159,\n",
       "        1167, 1170, 1190, 1194, 1202, 1247, 1280, 1319, 1323, 1388, 1402,\n",
       "        1417, 1446, 1448, 1466, 1467, 1489, 1495, 1497, 1523, 1527, 1584,\n",
       "        1609, 1617, 1652, 1681, 1683, 1706, 1762, 1766, 1795, 1818, 1820,\n",
       "        1831, 1848, 1888, 1926, 1934, 1938, 1946, 1948, 1952, 1967, 1977,\n",
       "        2077, 2094, 2100, 2105, 2111, 2115, 2155, 2168, 2182, 2193, 2216,\n",
       "        2258, 2352, 2362, 2373, 2387, 2395, 2431, 2478, 2498, 2523, 2564,\n",
       "        2599, 2631, 2634, 2639, 2647, 2678, 2695, 2700, 2721, 2722, 2728,\n",
       "        2737, 2744, 2749, 2755, 2767, 2777, 2782, 2810, 2826, 2837, 2840,\n",
       "        2842, 2848, 2851, 3011, 3020, 3048, 3056, 3067, 3073, 3090, 3099,\n",
       "        3111, 3120, 3133, 3166, 3204, 3222, 3249, 3259, 3278, 3279, 3282,\n",
       "        3288, 3306, 3325, 3333, 3341, 3394, 3396, 3404, 3409, 3434, 3442,\n",
       "        3471, 3494, 3503, 3533, 3537, 3563, 3573, 3618, 3628, 3706, 3707,\n",
       "        3711, 3717, 3720, 3743, 3752, 3764, 3776, 3786, 3796, 3799, 3805,\n",
       "        3819, 3831, 3833, 3842, 3850, 3862, 3886, 3889, 3894, 3918, 3922,\n",
       "        3923, 3937, 3959, 3978, 4056, 4058, 4077, 4092, 4114, 4116, 4127,\n",
       "        4143, 4144, 4146, 4148, 4151, 4161, 4171, 4178, 4188, 4195, 4237,\n",
       "        4252, 4308, 4357, 4359, 4367, 4387, 4404, 4423, 4425, 4459, 4539,\n",
       "        4569, 4590, 4640, 4641, 4663, 4675, 4676, 4708, 4710, 4729, 4731,\n",
       "        4738, 4743, 4744, 4774, 4811, 4815, 4859, 4861, 4874, 4876, 4899,\n",
       "        4917, 4922, 4923, 4928, 4933, 4957, 4972, 4986, 4997, 4999, 5002,\n",
       "        5012, 5025, 5026, 5067, 5084, 5110, 5142, 5178, 5184, 5214, 5215,\n",
       "        5232, 5233, 5249, 5260, 5263, 5280, 5291, 5299, 5323, 5327, 5339,\n",
       "        5371, 5375, 5386, 5445, 5456, 5470, 5490, 5507, 5544, 5572, 5581,\n",
       "        5613, 5626, 5627, 5631, 5692, 5693, 5709, 5721, 5758, 5772, 5773,\n",
       "        5779, 5781, 5792, 5812, 5820, 5832, 5835, 5836, 5840, 5851, 5865,\n",
       "        5882, 5886, 5889, 5896, 5904, 5915, 5940, 5942, 5974, 5992, 6001,\n",
       "        6003, 6036, 6042, 6055, 6087, 6106, 6108, 6109, 6112, 6113, 6114,\n",
       "        6122, 6127, 6158, 6163, 6164, 6166, 6184, 6224, 6229, 6240, 6251,\n",
       "        6266, 6269, 6273, 6281]),)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 details\n",
    "* max_words = 1100\n",
    "* batch_size = 32\n",
    "* epochs = 5\n",
    "* First layer 512 units \n",
    "* Dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15181 samples, validate on 3796 samples\n",
      "Epoch 1/5\n",
      "15181/15181 [==============================] - 2s 140us/step - loss: 0.6807 - acc: 0.7869 - val_loss: 0.3288 - val_acc: 0.8962\n",
      "Epoch 2/5\n",
      "15181/15181 [==============================] - 2s 125us/step - loss: 0.2650 - acc: 0.9162 - val_loss: 0.3053 - val_acc: 0.9002\n",
      "Epoch 3/5\n",
      "15181/15181 [==============================] - 2s 117us/step - loss: 0.1777 - acc: 0.9442 - val_loss: 0.3018 - val_acc: 0.8999\n",
      "Epoch 4/5\n",
      "15181/15181 [==============================] - 2s 121us/step - loss: 0.1198 - acc: 0.9625 - val_loss: 0.3152 - val_acc: 0.8996\n",
      "Epoch 5/5\n",
      "15181/15181 [==============================] - 2s 120us/step - loss: 0.0809 - acc: 0.9758 - val_loss: 0.3026 - val_acc: 0.9028\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(512, input_shape=(1000,)))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(10))#number of outputs\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model3.fit(snippet_x_train, snippet_y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_are_3 = model3.predict_classes(snippet_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub2 = np.subtract(predictions_are,predictions_are_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    7,   29,   30,   36,   55,   67,  140,  157,  180,  191,\n",
       "         232,  245,  258,  266,  272,  279,  333,  339,  348,  352,  393,\n",
       "         396,  411,  424,  427,  446,  451,  465,  489,  519,  570,  585,\n",
       "         616,  622,  640,  652,  655,  664,  694,  710,  720,  751,  754,\n",
       "         806,  842,  846,  862,  881,  896,  903,  920,  922,  929,  934,\n",
       "         961,  973, 1006, 1033, 1034, 1055, 1102, 1127, 1138, 1157, 1159,\n",
       "        1167, 1170, 1190, 1194, 1202, 1247, 1280, 1319, 1323, 1388, 1402,\n",
       "        1417, 1446, 1448, 1466, 1467, 1489, 1495, 1497, 1523, 1527, 1584,\n",
       "        1609, 1617, 1652, 1681, 1683, 1706, 1762, 1766, 1795, 1818, 1820,\n",
       "        1831, 1848, 1888, 1926, 1934, 1938, 1946, 1948, 1952, 1967, 1977,\n",
       "        2077, 2094, 2100, 2105, 2111, 2115, 2155, 2168, 2182, 2193, 2216,\n",
       "        2258, 2352, 2362, 2373, 2387, 2395, 2431, 2478, 2498, 2523, 2564,\n",
       "        2599, 2631, 2634, 2639, 2647, 2678, 2695, 2700, 2721, 2722, 2728,\n",
       "        2737, 2744, 2749, 2755, 2767, 2777, 2782, 2810, 2826, 2837, 2840,\n",
       "        2842, 2848, 2851, 3011, 3020, 3048, 3056, 3067, 3073, 3090, 3099,\n",
       "        3111, 3120, 3133, 3166, 3204, 3222, 3249, 3259, 3278, 3279, 3282,\n",
       "        3288, 3306, 3325, 3333, 3341, 3394, 3396, 3404, 3409, 3434, 3442,\n",
       "        3471, 3494, 3503, 3533, 3537, 3563, 3573, 3618, 3628, 3706, 3707,\n",
       "        3711, 3717, 3720, 3743, 3752, 3764, 3776, 3786, 3796, 3799, 3805,\n",
       "        3819, 3831, 3833, 3842, 3850, 3862, 3886, 3889, 3894, 3918, 3922,\n",
       "        3923, 3937, 3959, 3978, 4056, 4058, 4077, 4092, 4114, 4116, 4127,\n",
       "        4143, 4144, 4146, 4148, 4151, 4161, 4171, 4178, 4188, 4195, 4237,\n",
       "        4252, 4308, 4357, 4359, 4367, 4387, 4404, 4423, 4425, 4459, 4539,\n",
       "        4569, 4590, 4640, 4641, 4663, 4675, 4676, 4708, 4710, 4729, 4731,\n",
       "        4738, 4743, 4744, 4774, 4811, 4815, 4859, 4861, 4874, 4876, 4899,\n",
       "        4917, 4922, 4923, 4928, 4933, 4957, 4972, 4986, 4997, 4999, 5002,\n",
       "        5012, 5025, 5026, 5067, 5084, 5110, 5142, 5178, 5184, 5214, 5215,\n",
       "        5232, 5233, 5249, 5260, 5263, 5280, 5291, 5299, 5323, 5327, 5339,\n",
       "        5371, 5375, 5386, 5445, 5456, 5470, 5490, 5507, 5544, 5572, 5581,\n",
       "        5613, 5626, 5627, 5631, 5692, 5693, 5709, 5721, 5758, 5772, 5773,\n",
       "        5779, 5781, 5792, 5812, 5820, 5832, 5835, 5836, 5840, 5851, 5865,\n",
       "        5882, 5886, 5889, 5896, 5904, 5915, 5940, 5942, 5974, 5992, 6001,\n",
       "        6003, 6036, 6042, 6055, 6087, 6106, 6108, 6109, 6112, 6113, 6114,\n",
       "        6122, 6127, 6158, 6163, 6164, 6166, 6184, 6224, 6229, 6240, 6251,\n",
       "        6266, 6269, 6273, 6281]),)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(sub2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(predictions_are_3).to_excel(\"./subimission_03.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 performance\n",
    "* Val_Acc is 91.02\n",
    "* Score on submission 0.9111601291"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engg for basic NLP tasks \n",
    "### Count Vectorizer\n",
    "### TF-IDF\n",
    " * Word\n",
    " * N-Gram\n",
    " * Char Level\n",
    " \n",
    "### Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(data['cleaned'], data['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training length is 14232\n",
      "The training length is 4745\n"
     ]
    }
   ],
   "source": [
    "print(\"The training length is\",len(train_x))\n",
    "print(\"The training length is\",len(valid_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CV_features(alldata,train,val):\n",
    "    \n",
    "    # create a count vectorizer object \n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    count_vect.fit(alldata)\n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    train_df = count_vect.transform(train)\n",
    "    val_df = count_vect.transform(val)\n",
    "    \n",
    "    return train_df, val_df,count_vect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.58 s, sys: 144 ms, total: 5.73 s\n",
      "Wall time: 5.75 s\n"
     ]
    }
   ],
   "source": [
    "%time CV_train, CV_val, cv_obj = CV_features(data['cleaned'],train_x,valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40468"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_obj.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TF_word_features(alldata,train,val,mf=1000):\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=mf)\n",
    "    tfidf_vect.fit(alldata)\n",
    "    \n",
    "    TF_W_train = tfidf_vect.transform(train)\n",
    "    TF_W_val = tfidf_vect.transform(val)\n",
    "    \n",
    "    return TF_W_train, TF_W_val, tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.25 s, sys: 119 ms, total: 5.37 s\n",
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "%time TF_W_train, TF_W_val, TF_W_obj = TF_word_features(data['cleaned'],train_x,valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TF_Ng_features(alldata,train,val,mf=1000):\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}',ngram_range=(2,3),max_features=mf)\n",
    "    tfidf_vect.fit(alldata)\n",
    "    \n",
    "    TF_Ng_train = tfidf_vect.transform(train)\n",
    "    TF_Ng_val = tfidf_vect.transform(val)\n",
    "    \n",
    "    return TF_W_train, TF_W_val, tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.5 s, sys: 1.03 s, total: 40.5 s\n",
      "Wall time: 40.6 s\n"
     ]
    }
   ],
   "source": [
    "%time TF_Ng_train, TF_Ng_val, TF_Ng_obj = TF_Ng_features(data['cleaned'],train_x,valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TF_Chars_features(alldata,train,val,mf=1000):\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=mf)\n",
    "    tfidf_vect.fit(alldata)\n",
    "    \n",
    "    TF_Ch_train = tfidf_vect.transform(train)\n",
    "    TF_Ch_val = tfidf_vect.transform(val)\n",
    "    \n",
    "    return TF_Ch_train, TF_Ch_val, tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.8 s, sys: 1.16 s, total: 47.9 s\n",
      "Wall time: 48.3 s\n"
     ]
    }
   ],
   "source": [
    "%time TF_Ch_train, TF_Ch_val, TF_Ch_obj = TF_Chars_features(data['cleaned'],train_x,valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def train_model(alg, X, Y, val_X, val_y):\n",
    "    \n",
    "    alg.fit(X , Y)\n",
    "    \n",
    "    predictions = alg.predict(val_X)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), CV_train ,train_y ,CV_val ,valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB and Count_Vectors 95.13171759747102\n"
     ]
    }
   ],
   "source": [
    "print(\"NB and Count_Vectors\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB and TF_Words 82.31822971548999\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), TF_W_train ,train_y ,TF_W_val ,valid_y)\n",
    "print(\"NB and TF_Words\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB and TF_Ng 82.31822971548999\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), TF_Ng_train ,train_y ,TF_Ng_val ,valid_y)\n",
    "print(\"NB and TF_Ng\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB and TF_Chars 54.899894625922016\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), TF_Ch_train ,train_y ,TF_Ch_val ,valid_y)\n",
    "print(\"NB and TF_Chars\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB and Count_Vectors 89.6522655426765\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.GaussianNB(), CV_train.toarray() ,train_y ,CV_val.toarray() ,valid_y)\n",
    "print(\"NB and Count_Vectors\",accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.19 s, sys: 32.4 ms, total: 4.22 s\n",
      "Wall time: 4.23 s\n",
      "SVM and Count_Vectors 96.29083245521602\n"
     ]
    }
   ],
   "source": [
    "%time accuracy = train_model(svm.LinearSVC(), CV_train ,train_y ,CV_val ,valid_y)\n",
    "print(\"SVM and Count_Vectors\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 990 ms, sys: 22 ms, total: 1.01 s\n",
      "Wall time: 1.02 s\n",
      "SVM, C =0.01 and Count_Vectors 96.8809272918862\n"
     ]
    }
   ],
   "source": [
    "%time accuracy = train_model(svm.LinearSVC(C =0.01), CV_train ,train_y ,CV_val ,valid_y)\n",
    "print(\"SVM, C =0.01 and Count_Vectors\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 510 ms, sys: 37.1 ms, total: 548 ms\n",
      "Wall time: 590 ms\n",
      "SVM and TF_Words 90.32665964172814\n"
     ]
    }
   ],
   "source": [
    "%time accuracy = train_model(svm.LinearSVC(), TF_W_train ,train_y ,TF_W_val ,valid_y)\n",
    "print(\"SVM and TF_Words\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 866 ms, sys: 8.33 ms, total: 874 ms\n",
      "Wall time: 880 ms\n",
      "SVM and TF_Ng 92.07586933614331\n"
     ]
    }
   ],
   "source": [
    "%time accuracy = train_model(svm.LinearSVC(), TF_Ng_train ,train_y ,TF_Ng_val ,valid_y)\n",
    "print(\"SVM and TF_Ng\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.8 s, sys: 45.9 ms, total: 4.85 s\n",
      "Wall time: 4.85 s\n",
      "SVM and TF_Chars 87.98735511064278\n"
     ]
    }
   ],
   "source": [
    "%time accuracy = train_model(svm.LinearSVC(), TF_Ch_train ,train_y ,TF_Ch_val ,valid_y)\n",
    "print(\"SVM and TF_Chars\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(data['cleaned'])\n",
    "X_test = count_vect.transform(test_df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsvc_alg = svm.LinearSVC(C =0.01)\n",
    "lsvc_alg.fit(CV_train,train_y)\n",
    "\n",
    "predictions_lsvc = lsvc_alg.predict(X_test)\n",
    "\n",
    "pd.DataFrame(predictions_lsvc).to_excel(\"./subimission_07.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_alg = naive_bayes.MultinomialNB()\n",
    "NB_alg.fit(CV_train,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_MNB = NB_alg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "final_pred = np.array([])\n",
    "for i in range(0,len(test_df)):\n",
    "    final_pred = np.append(final_pred, stats.mode([predictions_are_3[i], predictions_lsvc[i], predictions_MNB[i]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6326"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6326,)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(final_pred).to_excel(\"./subimission_05.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mode([predictions_are_3[0], predictions_lsvc[0], predictions_MNB[0]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "param_grid = {'C': Cs}\n",
    "grid_search = GridSearchCV(svm.LinearSVC(), param_grid)\n",
    "grid_search.fit(CV_train, train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsvc_alg_2 = svm.LinearSVC(C=0.01)\n",
    "lsvc_alg_2.fit(CV_train,train_y)\n",
    "predictions_lsvc_2 = lsvc_alg_2.predict(X_test)\n",
    "pd.DataFrame(predictions_lsvc_2).to_excel(\"./subimission_06.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9574198988195616"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm0 = GradientBoostingClassifier(random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 53s, sys: 1.57 s, total: 14min 54s\n",
      "Wall time: 14min 56s\n"
     ]
    }
   ],
   "source": [
    "%time accuracy = train_model(gbm0,CV_train ,train_y ,CV_val ,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8741833508956797\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.3 s, sys: 236 ms, total: 33.5 s\n",
      "Wall time: 3min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=8,\n",
       "              max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=50, min_samples_split=500,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=10, subsample=0.8, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params={}, iid=False, n_jobs=4,\n",
       "       param_grid={'n_estimators': [20, 30, 40, 50, 60, 70, 80]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {'n_estimators':list(range(20,81,10))}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10), \n",
    "param_grid = param_test1, scoring='accuracy',n_jobs=4,iid=False, cv=5)\n",
    "%time gsearch1.fit(CV_train,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.68114, std: 0.01864, params: {'n_estimators': 20},\n",
       " mean: 0.73729, std: 0.01113, params: {'n_estimators': 30},\n",
       " mean: 0.78001, std: 0.00536, params: {'n_estimators': 40},\n",
       " mean: 0.80783, std: 0.00622, params: {'n_estimators': 50},\n",
       " mean: 0.82638, std: 0.00495, params: {'n_estimators': 60},\n",
       " mean: 0.84317, std: 0.00554, params: {'n_estimators': 70},\n",
       " mean: 0.85484, std: 0.00309, params: {'n_estimators': 80}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 587 ms, total: 1min 1s\n",
      "Wall time: 26min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "              presort='auto', random_state=10, subsample=0.8, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params={}, iid=False, n_jobs=4,\n",
       "       param_grid={'max_depth': [5, 7, 9, 11, 13, 15], 'min_samples_split': [200, 400, 600, 800, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {'max_depth':list(range(5,16,2)), 'min_samples_split':list(range(200,1001,200))}\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=80, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test2, scoring='accuracy',n_jobs=4,iid=False, cv=5)\n",
    "%time gsearch2.fit(CV_train,train_y)\n",
    "#print(gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.85308, std: 0.00245, params: {'max_depth': 5, 'min_samples_split': 200},\n",
       " mean: 0.85020, std: 0.00353, params: {'max_depth': 5, 'min_samples_split': 400},\n",
       " mean: 0.85132, std: 0.00477, params: {'max_depth': 5, 'min_samples_split': 600},\n",
       " mean: 0.85385, std: 0.00449, params: {'max_depth': 5, 'min_samples_split': 800},\n",
       " mean: 0.85427, std: 0.00279, params: {'max_depth': 5, 'min_samples_split': 1000},\n",
       " mean: 0.88147, std: 0.00358, params: {'max_depth': 7, 'min_samples_split': 200},\n",
       " mean: 0.88210, std: 0.00406, params: {'max_depth': 7, 'min_samples_split': 400},\n",
       " mean: 0.88189, std: 0.00531, params: {'max_depth': 7, 'min_samples_split': 600},\n",
       " mean: 0.88006, std: 0.00578, params: {'max_depth': 7, 'min_samples_split': 800},\n",
       " mean: 0.88168, std: 0.00432, params: {'max_depth': 7, 'min_samples_split': 1000},\n",
       " mean: 0.89250, std: 0.00268, params: {'max_depth': 9, 'min_samples_split': 200},\n",
       " mean: 0.89608, std: 0.00240, params: {'max_depth': 9, 'min_samples_split': 400},\n",
       " mean: 0.89671, std: 0.00236, params: {'max_depth': 9, 'min_samples_split': 600},\n",
       " mean: 0.89559, std: 0.00381, params: {'max_depth': 9, 'min_samples_split': 800},\n",
       " mean: 0.89630, std: 0.00634, params: {'max_depth': 9, 'min_samples_split': 1000},\n",
       " mean: 0.90444, std: 0.00259, params: {'max_depth': 11, 'min_samples_split': 200},\n",
       " mean: 0.90465, std: 0.00359, params: {'max_depth': 11, 'min_samples_split': 400},\n",
       " mean: 0.90402, std: 0.00361, params: {'max_depth': 11, 'min_samples_split': 600},\n",
       " mean: 0.90655, std: 0.00388, params: {'max_depth': 11, 'min_samples_split': 800},\n",
       " mean: 0.90374, std: 0.00492, params: {'max_depth': 11, 'min_samples_split': 1000},\n",
       " mean: 0.91231, std: 0.00376, params: {'max_depth': 13, 'min_samples_split': 200},\n",
       " mean: 0.91344, std: 0.00245, params: {'max_depth': 13, 'min_samples_split': 400},\n",
       " mean: 0.91105, std: 0.00285, params: {'max_depth': 13, 'min_samples_split': 600},\n",
       " mean: 0.91273, std: 0.00351, params: {'max_depth': 13, 'min_samples_split': 800},\n",
       " mean: 0.91393, std: 0.00332, params: {'max_depth': 13, 'min_samples_split': 1000},\n",
       " mean: 0.91491, std: 0.00370, params: {'max_depth': 15, 'min_samples_split': 200},\n",
       " mean: 0.91653, std: 0.00457, params: {'max_depth': 15, 'min_samples_split': 400},\n",
       " mean: 0.91758, std: 0.00499, params: {'max_depth': 15, 'min_samples_split': 600},\n",
       " mean: 0.91540, std: 0.00192, params: {'max_depth': 15, 'min_samples_split': 800},\n",
       " mean: 0.91533, std: 0.00300, params: {'max_depth': 15, 'min_samples_split': 1000}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 15, 'min_samples_split': 600}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9175823948263757"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
